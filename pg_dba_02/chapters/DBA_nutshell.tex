\chapter{DBA in a nutshell}
A database administrator is a strange combination of theory and practice. Because is a mix of strictness 
and loose rules is quite difficult to explain what exactly a DBA does. Working with databases requires 
passion and knowledge mixed with empathy and pragmatism, which helps to understand what 
the DBMS is thinking. With the commercial products the knowledge is commonly limited by the vendor's 
documentations and the missing parts are filled by the DBA's personal experience. Working with a free DBMS 
like PostgreSQL the source code's availability facilitates the knowledge acquisition. The PostgreSQL's 
codebase is superb poetry written in C. Just reading the comments builds an intimate relation between the 
cold binary programs and the geniuses behind the PostgreSQL Global Development Group.\newline

A day in the life of a DBA does not have fixed boundaries. The ``day'' can be just one day or spawn 
over several months, if for example if there are long procedures to run inside the well controlled 
maintenance windows. Each new day is different from the previous with a combination of various duties. 
Those are the daily tasks, the monitoring, the proactive thinking and the emergency handling. Let's see 
them in detail.

\section{Daily tasks}
Under the group of daily tasks there are all the procedures which are well consolidated on the documents 
and in the DBA mind. For example, configuring the PostgreSQL's memory parameters should be something 
immediate which does not require reading the docs. Any task in this category is successful if remains 
completely unnoticed. Is not unlikely for the production environment that the routine tasks are 
performed in antisocial hours like Sunday night or early morning in the working days.

\section{Monitoring}
A system without monitoring is an one way ticket to the disaster. Whether solution is used it should be 
something simple to configure and with a decent low rate of false positives. Having a nagging monitor is 
exactly the same like not having at all. The important alerts will pass completely unnoticed. Alongside 
with the general purpose solutions like nagios there is a new interesting PostgreSQL's dedicated called 
\href{http://opm.io/}{OPM}. Configuring a passive error detector like tail\_n\_mail is a very good idea as 
well. 

\section{Proactive thinking}
The proactive thinking is the distinctive mark between a good and DBA and a cheap professional. The 
capability of building a mental map for finding any possible issue, projected in the near future, can 
make the difference between spending the night sleeping or working frantically before the next day begins. 
Reacting to the issues is fine. Trying to prevent them is much better. This kind of tasks is strictly 
related with the monitoring. For example let's consider a master and a slave configured in continuous 
recovery. Because the slave is made copying the data files we should expect both having the same size. 
A difference in size between the master and the standby server can tell us that the filesystem have some 
issues or is not correctly configured. In this example, assuming the filesystem is XFS a bloating box is 
probably caused by the \href{
http://serverfault.com/questions/406069/why-are-my-xfs-filesystems-suddenly-consuming-more-space-and-full-of
-sparse-file}{dynamic EOF allocation} introduced in the newer XFS releases.

\section{Emergency handling}
\label{sec:DBAEMERGENCY}
Shit happens, deal with it. Sooner or later a disaster will strike requiring all the experience and 
efficiency of the database experts for putting back the system in production state. It doesn't matter if 
the issue is caused by an user screwing up a database table, a hardware failure or a fleet of Vogon 
spaceships ready to demolish the earth. The rule number zero when in handling an emergency is \textit{never 
guess}. Guessing without knowledge can lead to the database destruction.\newline

An example will give us a better understanding on this concept. Let's consider one of the most common 
causes of PostgreSQL's outage in the linux world, the distribution's upgrade. Usually when a linux 
distribution is upgraded is very likely that PostgreSQL will jump to a different major version. 
Because the data area is not compatible across the major releases this will prevent the cluster's to start. 
However the fix is quite simple and requires just installing the correct PostgreSQL's version.\newline

Few years ago a desperate user posted the same question on a message board. One of the ``expert'' 
guessed that pg\_resetxlog could be be useful in this case. Before reading further please read the 
\href{http://www.postgresql.org/docs/9.3/static/app-pgresetxlog.html}{pg\_resetxlog documentation}. 
Following the ``expert'' advice would transform a simple version mismatch's issue into a serious 
data area corruption. 

\section{Failure is not an option}
The failure is not an option. Despite this statement is quite pretentious is also the rule number 
zero for being a decent DBA. The task failure, should this be a simple alter table or an emergency 
restore, is not acceptable. The database is the core of any application and therefore is the most important 
element of the infrastructure.\newline

In order to achieve this impossible level of service any task should be considered as single shot. 
Everything must run perfectly at the first run like not having a rollback plan. However the rollback 
procedure must be prepared and tested alongside with the main task on the test environment in order to get a 
smooth restore of the previous state if required. It's also very important to keep the checklist in mind. 
Having a checklist document is good but this does not mean that the DBA should rely only on the 
document. Remembering the checklist's steps allow the DBA to catch the exact point of any possible failure 
ensuring a rapid fix when this happens.\newline

Finally, having a plan B gives to the DBA a multiple approach for the task if something goes wrong. For 
example, when dealing with normal size databases\footnote{I consider normal sized any database under the 500 
GB} is possible to implement many strategies for the disaster recovery getting the similar time needed for 
the recovery . When the amount of data becomes important this is no longer true. For example a logical 
restore takes more time than a point in time recovery or a standby failover. In this case the plan A is the 
physical restore and, if this does not work then there is the plan B, the logical recovery.

\section{RTFM}
The acronym RTFM stands for Read The Fucking Manual. It's quite obvious that reading the manual is the 
perfect approach for finding the correct information or at least a hint for finding the solution.\newline

The false confidence or a lack of humbleness make people to forget how important is to read the documents. 
Understanding the documents is the next step, and is not simple indeed. In particular if the background is 
poor or the reader have preconceptions which alter the word meanings.\newline

On the same message board mentioned in \ref{sec:DBAEMERGENCY} an user asked some details about the binary 
data. Another "expert" explained how PostgreSQL had two ways of storing the binary data. The bytea and the 
OID, which maximum size limit was respectively  one and two GB. I replied with a simple question. 
\textit{How is possible to store two GB in a four byte integer without strange voodoo 
rituals?}\newline

After an unpleasant argument finally the ``expert'' admitted he did not understood the mechanism used by 
PostgreSQL to workaround the size limit of the TOASTed datum. If you are curious, well, 
\href{http://www.postgresql.org/docs/9.3/static/largeobjects.html}{RTFM}!

\section{Wrap up}