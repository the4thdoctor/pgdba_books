\chapter{The logical layout}
\label{cha:LOGICLAY}\index{Logical layout}
In this we'll take a look to the PostgreSQL logical layout.
We'll start with the connection process. Then we'll see the logical relations like tables, indices
and views. The chapter will end with the tablespaces and the MVCC.

\section{The connection}
When a client starts a connection to a running cluster, the process pass through few steps. \newline

The first connection's stage is the check using the host based authentication. The cluster scans the
pg\_hba.conf file searching a  match for the connection's parameters. Those are, for example, the
client's host, the user etc. The host file is usually saved inside the the data area alongside the
configuration file postgresql.conf. The pg\_hba.conf is read from the top to the bottom and the
first matching row for the client's parameters is used to determine the authentication method to
use. If PostgreSQL reaches the end of the file without match the connection is refused.\newline

The pg\_hba.conf structure is shown in \ref{tab:PGHBA}

\begin{table}[H]
  \begin{tabular}{ccccc}
    Type & Database & User & Address & Method \\ 
    \hline
    local & name & name & ipaddress/network mask & trust\\
    host & * & * & host name & reject\\
    hostssl & &  &  & md5\\
    hostnossl & &  &  & password \\
    & & &  & gss \\
    & & &  & sspi \\
    & & &  & krb5 \\
    & & &  & ident \\
    & & &  & peer \\
    & & &  & pam \\
    & & &  & ldap \\
    & & &  & radius \\
    & & &  & cert \\
  \end{tabular}
  \caption{\label{tab:PGHBA}pg\_hba.conf}
\end{table}

The column type specifies if the connection is local or host. The former is when the connection is
made using a socket. The latter when the connection uses the network. It's also possible to
specify if the host connection should be secure or plain using hostssl and hostnossl.\newline

The Database and User columns are used to match specific databases and users.\newline

The column address have sense only if the connection is host, hostssl or hostnossl. The value can
be an ip address plus the network mask. Is also possible to specify the hostname. There is the
full support for ipv4 and ipv6.

The pg\_hba.conf's last column is the authentication method for the matched row. The action to
perform after the match is done. PostgreSQL supports many methods ranging from the plain password
challenge to kerberos.\newline

We'll now take a look to the built in methods.

\begin{itemize}
 \item \textbf{trust}: The connection is authorised without any further action. Is quite useful 
if the password is lost. Use it with caution.

\item \textbf{peer}: The connection is authorised if the OS user matches the 
database user. It's useful for the local connections. 

\item \textbf{password}: The connection establishes if the connection's user and the password
matches with the values stored in the pg\_shadow system table. This method sends the password in
clear text. Should be used only on trusted networks.

\item \textbf{md5}: This method is similar to password. It uses a better security encoding the
passwords using the md5 algorithm. Because md5 is deterministic, there is pseudo random
subroutine which prevents to have the same string sent over the network.

\item \textbf{reject}: The connection is rejected. This method is very useful to keep the sessions
out of the database. e.g. maintenance requiring single user mode.

\end{itemize}

When the connection establishes the postgres main process forks a new backend process attached to
the shared buffer. The fork process is expensive. This makes the connection a potential
bottleneck. Opening new connections can degrade the operating system performance and eventually
produce zombie processes. Keeping the connections constantly connected maybe is a reasonable fix.
Unfortunately this approach have a couple of unpleasant side effects.\newline

Changing any connection related parameter like the max\_connections, requires a cluster restart.
For this reason planning the resources is absolutely vital. For each connection present in
 max\_connections the cluster allocates 400 bytes of shared memory. For each connection established 
the cluster allocates a per user memory area wich size is determined by the parameter
work\_mem.\newline

For example let's consider a cluster with a shared\_buffer set to 512 MB and the work\_mem 
set to 100MB. Setting the max\_connections to only 500 requires a potentially 49 GB of total memory
if all the connections are in use. Because the work\_mem can affects the performances, its
value should be determined carefully. Being a per user memory any change to work\_mem does not
require the cluster's start but a simple reload.\newline 

In this kind of situations a connection pooler can be a good solutions. The sophisticated
\href{http://www.pgpool.net/}{pgpool}  or the
lightweight \href{http://pgfoundry.org/projects/pgbouncer/}{pgbouncer}  can help to boost the
connection's performance.\newline

By default a fresh data area initialisation listens only on the localhost. The GUC parameter
governing this aspect is listen\_addresses. In order to have the cluster accepting connections from
the rest of the network the values should change to the correct listening addresses specified
as values separated by commas. It's also possible to set it to * as wildcard.

Changing the parameters max\_connections and listen\_addresses require the cluster restart.



\section{Databases}
\label{sec:DATABASES}
Unlikely other DBMS, a PostgreSQL connection requires the database name in the connection string.
Sometimes this can be omitted in psql when this information is supplied in another way.\newline

When omitted psql checks if the environment variable \$PGDATABASE \index{\$PGDATABASE variable} is 
set. If \$PGDATABASE is missing then psql defaults the database name to connection's username. This 
leads to confusing error messages. For example, if we have a username named test but not a database 
named test the connection will fail even with the correct credentials.

\begin{verbatim}
postgres@tardis:~$ psql -U test -h localhost
Password for user test: 
psql: FATAL:  database "test" does not exist
\end{verbatim}

This error appears because the pg\_hba.conf allow the connection for any database. Even for a not
existing one. The connection is then terminated when the backend ask to connect to the database 
named test which does not exists.\newline

This is very common for the new users. The solution is incredibly simple because in a PostgreSQL 
cluster there are at least three databases. Passing the name template1 as last parameter will do 
the trick.

\begin{verbatim}
postgres@tardis:~$ psql -U test -h localhost template1
Password for user test: 
psql (9.3.4)
SSL connection (cipher: DHE-RSA-AES256-SHA, bits: 256)
Type "help" for help.
\end{verbatim}

When the connection is established we can query the system table pg\_database to get the 
cluster's database list. 

\begin{lstlisting}[style=pgsql]
template1=> SELECT datname FROM pg_database;
    datname    
---------------
 template1
 template0
 postgres
(3 rows)

\end{lstlisting}

Database administrators coming from other DBMS can be confused by the postgres database.
This database have nothing special. Its creation was added since the version 8.4 because it was 
useful to have it. You can just ignore it or use it for testing purposes. Dropping the postgres 
database does not corrupts the cluster. Because this database is often used by third party 
tools before dropping it check if is in use in any way.\newline

The databases template0 and template1 \index{template1 database} \index{template0 
database} like the name suggests are the template databases. A template database \index{template 
database} is used to build new database copies via the physical file copy. 

When initdb initialises the data area the database template1 is populated with the correct
references to the WAL records, the system views and the procedural language PL/PgSQL. When
this is done the database template0 and the postgres databases are then created using the template1
database.

The database template0 doesn't allow the connections. It's main usage is to rebuild the
database template1 if it gets corrupted or for creating databases with a character encoding/ctype, 
different from the cluster wide settings. 
\index{CREATE DATABASE}

\begin{lstlisting}[style=pgsql]
postgres=# CREATE DATABASE db_test WITH ENCODING 'UTF8' LC_CTYPE 'en_US.UTF-8';
ERROR:  new LC_CTYPE (en_US.UTF-8) is incompatible with the LC_CTYPE of the 
template database (en_GB.UTF-8)
HINT:  Use the same LC_CTYPE as in the template database, or use template0 as 
template.

postgres=# CREATE DATABASE db_test WITH ENCODING 'UTF8' LC_CTYPE 'en_US.UTF-8' 
TEMPLATE template0;
CREATE DATABASE
postgres=# 

\end{lstlisting}

If the template is omitted the CREATE DATABASE statement will use template1 by default. 


A database can be renamed or dropped with ALTER DATABASE and DROP DATABASE \index{ALTER 
DATABASE}\index{DROP DATABASE} statements. Those operations require the exclusive access to the 
affected database. If there are connections established the drop or rename will fail.

\begin{lstlisting}[style=pgsql]
postgres=# ALTER DATABASE db_test RENAME TO db_to_drop;
ALTER DATABASE

postgres=# DROP DATABASE db_to_drop;
DROP DATABASE

\end{lstlisting}




\section{Tables}\index{Tables}
\label{sec:TABLES}
In our top down approach to the PostgreSQL's logical model, the next step is the relation.
In the PostgreSQL jargon a relation is an object which carries the data or the way to
retrieve the data. A relation can have a physical counterpart or be purely logical. We'll take a 
look in particular to three of them starting with the tables.\newline

A table is the fundamental storage unit for the data. PostgreSQL implements many kind of tables
with different levels of durability. A table is created using the SQL command CREATE TABLE. The data
is stored into a table without any specific order. Because the MVCC implementation a row update can
change the row's physical position. For more informations look to \ref{sec:MVCC}. PostgreSQL
implements three kind of tables.

\subsection{Logged tables}\index{Logged tables}
By default CREATE TABLE creates a logged table. This kind of table implements the durability
logging any change to the write ahead log. The data pages are loaded in the shared buffer and any
change to them is logged first to the WAL. The consolidation to the the data file happens later. 

\subsection{Unlogged tables}\index{Unlogged tables}
\label{sub:UNLOGGEDTABLES}
An unlogged table have the same structure like the logged table. The difference is such kind of 
tables are not crash safe. The data is still consolidated to the data file but the pages modified 
in memory do not write their changes to the WAL. The main advantage is the write operations which 
are considerably faster at the cost of the data durability. The data stored into an ulogged table
should be considered partially volatile. The database will truncate those tables when the crash
recovery occurs. Because the unlogged table don't write to the WAL, those tables are not accessible 
on a physical standby. 

\subsection{Temporary tables}\index{Temporary tables}
A temporary table is a relation which lives into the backend's local memory. When the connection 
ends the table is dropped. Those table can have the same name for all the sessions because 
they are completely isolated. If the amount of data stored into the table is lesser than 
the temp\_buffers value the table will fit in memory with great speed advantage. Otherwise the 
database will create a temporary relation on disk. The parameter temp\_buffers can be altered for 
the session but only before the first temporary table is created. 


\subsection{Foreign tables}\index{Foreign tables}
The foreign tables were first introduced with PostgreSQL 9.1 as read only relations, improving 
considerably the DBMS connectivity with other data sources. A foreign table works exactly like a
local table. A foreign data wrapper interacts with the foreign data source and handles the
data flow.\newline

There are many different foreign data wrappers available for very exotic data sources. From the
version 9.3 the postgres\_fdw becomes available and the the foreign tables are writable. The
implementation of the postgres\_fdw implementation is similar to old dblink module with a more
efficient performance management and the connection's caching.

\section{Table inheritance}\index{Table inheritance}
PostgreSQL is an Object Relational Database Management System rather a simple DBMS. Some of the
concepts present in the object oriented programming are implemented in the PostgreSQL logic. The
relations are also known as classes and the table's columns as attributes. \newline

The table inheritance is a logical relationship between a parent table and one or more child 
tables. The child table inherits the parent's attribute structure but not the physical
storage.\newline 


Creating a parent/child structure is straightforward.

\begin{lstlisting}[style=pgsql]
db_test=#CREATE TABLE t_parent
                      (
                          i_id_data     integer,
                          v_data        character varying(300)
                      );

CREATE TABLE                     

db_test=#CREATE TABLE t_child_01
                      
                      ()
             INHERITS (t_parent)
                      ;                      
db_test=# \d t_parent
            Table "public.t_parent"
  Column   |          Type          | Modifiers 
-----------+------------------------+-----------
 i_id_data | integer                | 
 v_data    | character varying(300) | 
Number of child tables: 1 (Use \d+ to list them.)

db_test=# \d t_child_01 
           Table "public.t_child_01"
  Column   |          Type          | Modifiers 
-----------+------------------------+-----------
 i_id_data | integer                | 
 v_data    | character varying(300) | 
Inherits: t_parent

\end{lstlisting}

The inheritance is usually defined at creation time. It's possible to enforce the inheritance
between two existing tables with the ALTER TABLE ... INHERIT command. The two table's structure
must be identical.

\begin{lstlisting}[style=pgsql]

db_test=# ALTER TABLE t_child_01 NO INHERIT t_parent;
ALTER TABLE
db_test=# ALTER TABLE t_child_01 INHERIT t_parent;
ALTER TABLE

\end{lstlisting}

Because the physical storage is not shared then the unique constraints aren't globally enforced on
the inheritance tree. This prevents the creation of any global foreign key. Using the table
inheritance, combined with the constraint exclusion and the triggers/rules, is a partial workaround
for the table partitioning.

\section{Indices}
An index is a structured relation. The indexed entries are used to access the tuples stored in the
tables. The index entries are the actual data with a pointer to the corresponding table's pages.\newline

It's important to bear in mind that the indices add overhead to the write operations. Creating an index
does not guarantee its usage.The cost based optimiser, for example, can simply consider the index access
more expensive than a sequential access. The stats system views, like the pg\_stat\_all\_indexes,
store the usage counters for the indices.\newline

For example this simple query finds all the indices in the public schema with index scan counter zero.

\begin{lstlisting}[style=pgsql]
SELECT
        schemaname,
        relname,
        indexrelname,
        idx_scan
FROM
         pg_stat_all_indexes
WHERE
                schemaname='public'
        AND     idx_scan=0
;

\end{lstlisting}

Having an efficient maintenance plan can improve sensibly the database performance. Take a look to 
\ref{cha:MAINTENANCE} for more information.

PostgreSQL implements many kind of indices. The keyword USING specifies the index type at create time.

\begin{lstlisting}[style=pgsql]
 CREATE INDEX idx_test ON t_test USING hash (t_contents);
\end{lstlisting}

If the clause USING is omitted the index defaults to the B-tree.

\subsection{b-tree}
The general purpose B-tree\index{index,b-tree} index implements the Lehman and Yao's
high-concurrency B-tree management algorithm. The B-tree can handle equality and range queries 
returning ordered data. The indexed values are stored into the index pages with the pointers to the
table's pages. Because the index is a relation not TOASTable the max length for an indexed key is 1/3
of the page size. More informations about TOAST are in \ref{sec:TOAST} \newline

\subsection{hash}
The hash indices\index{index,hash} can handle only equality and are not WAL logged. Their changes
are not replayed if the crash recovery occurs and do not propagate to the standby servers.\newline

\subsection{GiST}
The GiST indices\index{index,GiST} are the Generalised Search Tree. The GiST is a collection of
indexing strategies organised under a common infrastructure. They can implement arbitrary indexing
schemes like B-trees, R-trees  or other. The default installation comes with operator classes working on 
two elements geometrical data and for the nearest-neighbour searches. The GiST indices do not perform an
exact match. The false positives are removed with second rematch on the table's data.\newline

\subsection{GIN}
The GIN indices \index{index,GIN} are the Generalised Inverted Indices. This kind of index
is optimised for indexing the composite data types, arrays and vectors like the full text search
elements. This is the only index supported by the range types. The GIN are exact indices, when scanned
the returned set doesn't require recheck.

\section{Views}
\index{views}
\label{sec:VIEWS}
A view is a relation composed by a name and a query definition. This permits a faster access to complex
SQL. When a view is created the query is validated and all the objects involved are translated to their
binary representation. All the wildcards are expanded to the corresponding field's list.\newline

A simple example will help us to understand better this important concept. Let's create a table
populated using the function generate\_series(). We'll then create a view with a simple SELECT * from the
original table.

\begin{lstlisting}[style=pgsql]


CREATE TABLE t_data 
        ( 
                i_id            serial,
                t_content       text
        );

ALTER TABLE t_data 
ADD CONSTRAINT pk_t_data PRIMARY KEY (i_id);


INSERT INTO t_data
        (
                t_content
        )
SELECT
        md5(i_counter::text)
FROM
        (
                SELECT
                        i_counter
                FROM
                        generate_series(1,200) as i_counter
        ) t_series;

CREATE OR REPLACE VIEW v_data 
AS 
  SELECT 
          *
  FROM 
        t_data;


\end{lstlisting}

We can select from the view and from the the table with a SELECT and get the same data. The view's
definition in pg\_views shows no wildcard though. 


\begin{lstlisting}[style=pgsql]
db_test=# \x
db_test=# SELECT * FROM pg_views where viewname='v_data';
-[ RECORD 1 ]--------------------
schemaname | public
viewname   | v_data
viewowner  | postgres
definition |  SELECT t_data.i_id,
           |     t_data.t_content
           |    FROM t_data;


\end{lstlisting}

If we add a new field to the table t\_data this will not applies to the view.

\begin{lstlisting}[style=pgsql]
 ALTER TABLE t_data ADD COLUMN d_date date NOT NULL default now()::date;
 
 db_test=# SELECT * FROM t_data LIMIT 1;
 i_id |            t_content             |   d_date   
------+----------------------------------+------------
    1 | c4ca4238a0b923820dcc509a6f75849b | 2014-05-21
(1 row)


db_test=# SELECT * FROM v_data LIMIT 1;
 i_id |            t_content             
------+----------------------------------
    1 | c4ca4238a0b923820dcc509a6f75849b
(1 row)


 
\end{lstlisting}

Using the statement CREATE OR REPLACE VIEW we can put the view in sync with the table's structure.

\begin{lstlisting}[style=pgsql]
 CREATE OR REPLACE VIEW v_data 
AS 
  SELECT 
        *
  FROM 
        t_data;
        
db_test=# SELECT * FROM v_data LIMIT 1;
 i_id |            t_content             |   d_date   
------+----------------------------------+------------
    1 | c4ca4238a0b923820dcc509a6f75849b | 2014-05-21
(1 row)

\end{lstlisting}

Using the wildcards in the queries is a bad practice for many reasons. The potential outdated
match between the physical and the logical relations is one of those.\newline

The way PostgreSQL implements the views guarantee they never invalidate when the referred objects are
renamed. \newline

If new attributes needs to be added to the view the CREATE OR REPLACE statement can be used but only if
the fields are appended. If a table is referred by a view the drop is not possible. It's still possible
to drop a table with all the associated views using the clause CASCADE. This is a dangerous
practice though. The dependencies can be very complicated and a not careful drop can result in a
regression. The best approach is to check for the dependencies using the table pg\_depend.\newline

Storing a complex SQL inside the database avoid the overhead caused by the round trip between the client
and the server. A view can be joined with other tables or views. This practice is generally bad because
the planner can be confused by mixing different queries and can generate not efficient execution
plans.\newline

A good system to spot a view when writing a query is to use a naming convention. For example adding a v\_
in the view names and the t\_ in the table names will help the database developer to avoid
mixing logical an physical objects when writing SQL. Look to \ref{cha:COUPLETHINGS} for more information.

\index{view, updatable}
PostgreSQL from the version 9.3 supports the updatable views. This feature is limited just to the
simple views. A view is defined simple when the following is true.

\begin{itemize}


 \item  Does have exactly one entry in its FROM list, which must be a table or another updatable view.

 \item Does not contain WITH, DISTINCT, GROUP BY, HAVING,LIMIT, or OFFSET clauses at the top level.

 \item  Does not contain set operations (UNION, INTERSECT or EXCEPT) at the top level
 
 \item   All columns in the view's select list must be simple references to columns of the
underlying relation. They cannot be expressions, literals or functions. System columns cannot be
referenced, either.

 \item   Columns of the underlying relation do not appear more than once in the view's select list.

 \item   Does not have the security\_barrier property.

\end{itemize}

A complex view can still become updatable using the triggers or the rules.\newline

\index{view, materialised}
Another feature introduced by the 9.3 is the materialised views. This acts like a physical snapshot of
the saved SQL. The view's data can be refreshed with the statement REFRESH MATERIALIZED VIEW.  


\section{Tablespaces}\index{tablespaces,logical}
\label{sub:TBS-LOGICAL}
A tablespace\index{tablespace} is a logical name pointing to a physical location. 
This feature was introduced with the release 8.0 and its implementation did not change since
then. From the version 9.2 was introduced the function pg\_tablespace\_location(tablespace\_oid)
which gets the tablespace's location from the OID making the dba life easier.

When a new relation is created without the tablespace specification this defaults to the GUC
parameter default\_tablespace or to the database's default tablespace. 
On a fresh initialised cluster the initial value for the default tablespace is pg\_default which
points to to the directory \$PGDATA/base . Alongside with pg\_default there is the pg\_global as
well. This tablespace is reserved for the shared objects and points to the directory
\$PGDATA/global.

Creating a new tablespace is very simple. The physical location must be owned by the postgres
process user. 

For example let's create a tablespace pointing to the folder /var/lib/postgresql/pg\_tbs/ts\_test
with the name ts\_test.

\begin{lstlisting}[style=pgsql]
CREATE TABLESPACE ts_test 
OWNER postgres
LOCATION '/var/lib/postgresql/pg_tbs/ts_test' ;

\end{lstlisting}

Only superusers can create tablespaces. The OWNER clause is optional, if 
omitted the tablespace's owner defaults to the database user logged in. The tablespaces are cluster
wide and are listed into the pg\_tablespace system table.\newline

Creating a new relation into the tablespace ts\_test requires the TABLESPACE clause followed by the
tablespace name.

\begin{lstlisting}[style=pgsql]
CREATE TABLE t_ts_test
        (
                i_id serial,
                v_value text
        )
TABLESPACE ts_test ;

\end{lstlisting}

A relation can be moved from a tablespace to another using the ALTER command. For example let's
move the table t\_ts\_test to the  pg\_default tablespace.

\begin{lstlisting}[style=pgsql]
ALTER TABLE t_ts_test SET TABLESPACE pg_default;
\end{lstlisting}

Changing the relation's tablespace is transaction safe but requires an exclusive lock on the
affected relation. The lock prevents accessing the relation's data for the time required by the
move.If the relation have a significant size this should be considered carefully. Because the
exclusive lock conflicts with the locks issued by the backup, it's not possible to change the
relation's tablespace when pg\_dump is running.\newline


A tablespace can be removed with DROP TABLESPACE command. The tablespace must be empty before the
drop. There's no CASCADE clause to have the tablespace's contents dropped with the tablespace.

\begin{lstlisting}[style=pgsql]
postgres=# DROP TABLESPACE ts_test;
ERROR:  tablespace "ts_test" is not empty

postgres=# ALTER TABLE t_ts_test SET TABLESPACE pg_default;
ALTER TABLE
postgres=# DROP TABLESPACE ts_test;
DROP TABLESPACE

\end{lstlisting}


The tablespace feature adds flexibility to the space management. Even if not sophisticated like
other DBMS this implementation, combined with a careful design can improve sensibly the
performances.\newline

In \ref{sub:TBS-PHYSICAL} we'll take a look to the how PostgreSQL implements the tablespaces on the
physical side.



\section{Transactions}
\label{sec:TRANSACTION}
\index{transactions}
The atomicity the consistency and the isolation is implemented in PostgreSQL via the
MVCC\index{MVCC}. This is the acronym for Multi Version Concurrency Control\index{Multi Version
Concurrency Control}. This solution offers high efficiency in the concurrent user access for read
and write queries.\newline 

When a transaction starts a write operation receives an identifier, the XID \index{XID}. This is
a 32 bit quantity and is used to determine the transaction's visibility. All the transactions with
XID lesser than the current XID and committed are considered in the past and then visible. All the
transactions with XID greater than the current XID are in the future and not
visible. The not committed transactions are considered invisible as well.\newline

This comparison happens at tuple level using two system fields xmin and xmax. When a transaction
creates a new tuple the xmin is set with the transaction's xid. This field is also referred as the
insert's transaction id. When a tuple is deleted then the xmax value is updated to the transaction's
xid. The tuple when deleted is left in place, ensuring the read consistency for any transaction
which should see the previous version. The last tuple's version is a live tuple. The tuples which
versions are outdated are called dead tuples.\newline

There is no field for the update. In PostgreSQL the update is a a complete new insert. The update's
transaction id is used either for the new tuple's xmin and the old tuple's xmax. The dead tuples are
removed by VACUUM if no longer needed by existing transactions. In \ref{sec:TUPLES}
there is the tuple structure's description.\newline

Alongside with xmin and xmax there are two other system fields, the cmin and cmax. Their data
type is the command id, CID. Those fields are similar to xmin and xmax. They are used to track the
internal transaction's commands in order to avoid the command to be executed on the same
tuple multiple times. This solve the database's Halloween Problem described there
\href{http://en.wikipedia.org/wiki/Halloween_Problem}{
http://en.wikipedia.org/wiki/Halloween\_Problem}.\newline

The SQL standard defines four level of transaction's isolation levels where 
some phenomena are permitted or forbidden.
\index{transactions, isolation levels}

\begin{itemize}
 \item \textbf{dirty read}, when a transaction reads the data written by a concurrent 
uncommitted transaction

\item \textbf{nonrepeatable read} when a transaction re reads the same data and finds it changed by
another transaction which has committed since the initial read

\item \textbf{phantom read} when a transaction re executes a query returning a set 
of rows for a search condition and finds that results are changed by a recently committed 
transaction

\end{itemize}

The table \ref{tab:TRNISOLATION} shows the transaction's isolation levels with the phenomena
possible. In PostgreSQL it's possible to set all the four isolation levels but only the three
strictest levels are supported. Setting the  isolation level to read uncommited is
equivalent to set it to the read committed.

\begin{table}[H]
  \begin{tabular}{cccc}
    Isolation Level & Dirty Read    &    Nonrepeatable Read   &   Phantom 
Read\\ 
    \hline
    Read uncommitted  &  Possible    &    Possible     &   Possible\\
    Read committed    &  Not possible &  Possible     &   Possible\\
    Repeatable read   &  Not possible  & Not possible  &  Possible\\
    Serializable      &  Not possible  & Not possible   & Not possible\\
  \end{tabular}
  \caption{\label{tab:TRNISOLATION}SQL Transaction isolation levels}
\end{table}

The default the global isolation level is read committed. However is possible to change the
isolation level per session with the command:
\begin{lstlisting}[style=pgsql]
SET TRANSACTION ISOLATION LEVEL { SERIALIZABLE | REPEATABLE READ | READ 
COMMITTED | READ UNCOMMITTED }; 
\end{lstlisting}

Changing the default transaction isolation for the entire cluster is made using the GUC parameter
transaction\_isolation.


\subsection{Snapshot exports}
\label{sub:SNAPEXPORT}\index{transactions, snapshot export}
PostgreSQL 9.2 introduced the transaction's snapshot exports. Meanwhile a session is in
transaction its snapshot can be exported to other sessions. The snapshot is valid as long as the
exporting transaction is in progress. This feature opens many scenarios where multiple backends
can see a consistent snapshot. One of the first implementation is the brilliant parallel export in 
the 9.3's pg\_dump as described in \ref{sec:PGDUMPINT}.\newline

An example will explain better the concept. We'll use the table created in \ref{sec:VIEWS}.
The first thing to do is connecting to the cluster and start a new transaction. The function
pg\_export\_snapshot() creates a new snapshot and returns its identifier.

\begin{lstlisting}[style=pgsql]
postgres=# BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;
BEGIN
postgres=# SELECT pg_export_snapshot();
 pg_export_snapshot 
--------------------
 00001369-1
(1 row)

postgres=# SELECT count(*) FROM t_data;
 count 
-------
   200
(1 row)

\end{lstlisting}

It's important to specify the isolation level to REPEATABLE READ. 


With another backend we'll delete all the rows from the table t\_data table.

\begin{lstlisting}[style=pgsql]
postgres=# DELETE FROM t_data;
DELETE 200
postgres=# SELECT count(*) FROM t_data;
 count 
-------
     0
(1 row)

\end{lstlisting}

In the same backend let's import the snapshot using the identifier.

\begin{lstlisting}[style=pgsql]
postgres=# BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;
BEGIN
postgres=# SET TRANSACTION SNAPSHOT '00001369-1';
SET
postgres=# SELECT count(*) FROM t_data;
 count 
-------
   200
(1 row)

\end{lstlisting}

The rows are back again.

The import is possible only until the end of the 
transaction that exported it. The export requires at least the REPEATABLE READ as isolation level.
Using the READ COMMITTED for the export does not generates an error. However the snapshot is
discarded immediately because the READ COMMITTED takes a new snapshot for each command.

